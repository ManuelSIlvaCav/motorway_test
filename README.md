# Getting started

To get the application running stand on the root folder where the docker-compose is located and run the command

> docker-compose build

> docker-compose up

This should create 2 containers

- `motorway-test-db` is the database instance running with postgres
- `motorway-test-server` is the server for the app running with nodejs

You may now access and test the API for example with

> curl --location 'localhost:3000/api/v1/vehicles/2?ts=2024-09-11%2018%3A30%3A00'

The api follows the structue

> {domain}/api/v1/vehicles/:id?ts={date}

Where

- `/api/v1` makes sure there is a versioning scheme
- `/vehicles` resources for consistency on API request on vehicles domain
- `/:id` parameter to look for a particular vehicle
- QueryParams `?ts={timestamp}` to pass on a filter of date It has to be a date with format YYYY-MM-DD HH:MM:SS. If no filter is passed current date and time is used. If it is malformed a bad request will be raised.

# Dependencies

The api was developed with Nodejs and Express written on Typescript

Yarn was used as a package manager. The yarn.lock file is used for keeping dependencies in order.

- Nodemon - for hot reload and make development easier
- Postgres - driver (pg)
- TypeORM - to handle interactions with postgres database, build queries and manage entity relations
- Pino - Light weight logging package for structured logging
- Jest - Testing framework library

# Production ready

To make sure it is production ready some changes were made from a just localhost developed app, some of them are

- Dockerfile image generation optimized based on a 2 step build to end with an image with less size for deployment
- Migration file to keep updates to database. As is the database is not indexed for example on vehicleId as a foreign Key or Timestamp as an interesting lookup field. The migration aims to solve this.
- Logging structured with light weight library `pino` for better tracking
- Test suitecase with `jest` to be checked before deploying
  -- Can run test with `yarn test` from the package.json script

# Query and algorithm explained

The query done on SQL follows an algorithm needed to obtain the record that is the closest to the date input
Let it be `t` for the time input and the vehicleId `vid`

- Find the latest time lesser than the input `t` (a.k.a predecesor)
- Find the earliest time greater than the input `t` (a.k.a succesor)
- Find the distance between input `t` and both predecesor and succesor time
  -- Keep the minimum as the time of the that vehicle as the one to be returned
- Return the information of that stateLog complemented with the information for that VehicleId

This algorithm should be followed by any other approach to obtain the register that is being looked for.

# Further optimizations

The vehicle retrieval is a task delegated to the database by queries which could be underperformant later on once the row number increases.

Depending on the use case, the need for scalability and amount of states this solution could change.

For example if this is a real time access for information of vehicles maybe a time series database could be a better option to optimize retrievals.

If the usage is normal but the performance of the database is not scaling (ex. row number increses on the millions) an option could be to obtain the statelogs, then perform the search in memory and finally make a lookup for vehicle information. This is under the assumption that the statelogs of a vehicle are not that many, thus the amount of states to check should not be a problem.

Another improvement could be to use a document no-sql database to keep all the statelogs of a vehicle and you can retrieve that statelogs and the vehicle information on the same place then on memory make the search for the most recent state, without relaying so much on the database queries.

# Next steps

This would be the next steps in case a production ready app is needed.

- Build a terraform architecture with code to deploy this app (ex. AWS with ECS fargate, ECR repository and managed postgres DB)
- Build a pipeline to deploy the image generated by the Dockerfile to a remote repository (ex. Github actions)
  -- Within this pipeline include a testing step to test the tests in place to make sure it does not fail and check for coverage
  -- Include more integration tests and e2e testing.
- Deploy and make sure to include a monitoring system could be Cloudwatch in AWS or a third party provider ex. NewRelic
  -- On line with this build a sessionId that goes on every request to keep track of same requests errors across the app
- Build an API doc for end users to integrate or internal teams to know the work arround. Could be an OpenAPI spec or a SaaS documentation for API docs.
